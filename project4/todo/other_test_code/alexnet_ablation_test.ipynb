{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":78042,"databundleVersionId":8558952,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T17:18:10.470984Z","iopub.execute_input":"2024-06-10T17:18:10.471960Z","iopub.status.idle":"2024-06-10T17:18:10.969829Z","shell.execute_reply.started":"2024-06-10T17:18:10.471925Z","shell.execute_reply":"2024-06-10T17:18:10.968865Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom torch.utils.data import DataLoader, TensorDataset, ConcatDataset\nfrom sklearn.model_selection import train_test_split\n\n# GPU device\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:18:10.971389Z","iopub.execute_input":"2024-06-10T17:18:10.971790Z","iopub.status.idle":"2024-06-10T17:18:16.355789Z","shell.execute_reply.started":"2024-06-10T17:18:10.971765Z","shell.execute_reply":"2024-06-10T17:18:16.355008Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# get file name\ntestFileName = list()\ntrainFileNameClass0 = list()\ntrainFileNameClass1 = list()\nos.chdir('/kaggle/input/nycu-ml-pattern-recognition-hw-4/released/test')\ntestFileName = os.listdir()\nos.chdir('../train/class_0')\ntrainFileNameClass0 = os.listdir()\nos.chdir('../class_1')\ntrainFileNameClass1 = os.listdir()\nos.chdir('../../')\n# test file name\ntestFileName[0], trainFileNameClass0[0], trainFileNameClass1[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:18:16.357154Z","iopub.execute_input":"2024-06-10T17:18:16.357713Z","iopub.status.idle":"2024-06-10T17:18:16.371990Z","shell.execute_reply.started":"2024-06-10T17:18:16.357676Z","shell.execute_reply":"2024-06-10T17:18:16.371167Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('7c08c8c0-0064-40c3-a7cd-dda2b4043347.pkl',\n '32767294-e76e-485e-91a0-c2a123498e01.pkl',\n '3cc6a03d-7585-48f4-82cc-89448620b059.pkl')"},"metadata":{}}]},{"cell_type":"code","source":"def readData(filepath, filenamelist):\n    FileExtensionRemove = []\n    with open(filepath+filenamelist[0], 'rb') as f:\n        data = np.array([pickle.load(f)])\n    FileExtensionRemove.append(filenamelist[0][:-4])\n    for filename in filenamelist[1:]:\n        with open(filepath+filename, 'rb') as f:\n            data = np.concatenate((data, np.array([pickle.load(f)])), axis=0)\n        FileExtensionRemove.append(filename[:-4])\n    return data, FileExtensionRemove\n\ndef DataPreprocess(trainX0, trainX1=None):\n    # combine train data\n    if trainX1 is None:\n        trainX = trainX0\n    else:\n        trainX = np.concatenate((trainX0, trainX1))\n    # Transform to 0 ~ 1\n    trainX = trainX.astype('float32')\n    trainX /= 255\n    # To pytorch\n    trainX = torch.from_numpy(trainX)\n    if trainX1 is None:\n        trainY = None\n    else:\n        trainY = torch.concatenate((torch.zeros((trainX0.shape[0],1)), \n                                    torch.ones((trainX1.shape[0],1))))\n    return trainX, trainY","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:18:16.373949Z","iopub.execute_input":"2024-06-10T17:18:16.374295Z","iopub.status.idle":"2024-06-10T17:18:16.387882Z","shell.execute_reply.started":"2024-06-10T17:18:16.374264Z","shell.execute_reply":"2024-06-10T17:18:16.387221Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"DataSizeBatch = 25\ntrain_loader = []\nval_loader = []\nfor i in range(int(len(trainFileNameClass0)/DataSizeBatch)):\n    print(\"read batch\", i)\n    startIndex = i * DataSizeBatch\n    trainX0, _ = readData('train/class_0/', trainFileNameClass0[startIndex:startIndex + DataSizeBatch])\n    trainX1, _ = readData('train/class_1/', trainFileNameClass1[startIndex:startIndex + DataSizeBatch])\n    \n    trainX, trainY = DataPreprocess(trainX0, trainX1)\n    # Split data into train and validation sets\n    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.2)\n    \n    # Create DataLoader for batching\n    train_dataset = TensorDataset(trainX, trainY)\n    val_dataset = TensorDataset(valX, valY)\n\n    train_loader.append(DataLoader(train_dataset, batch_size=4, shuffle=True))\n    val_loader.append(DataLoader(val_dataset, batch_size=4, shuffle=False))\n    \n\n# Try to merge dataloader\n#trainLoader = DataLoader(ConcatDataset([dsa, dsb]))\n#valLoader = DataLoader(ConcatDataset([dsa, dsb]))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:18:16.389075Z","iopub.execute_input":"2024-06-10T17:18:16.389495Z","iopub.status.idle":"2024-06-10T17:19:19.023706Z","shell.execute_reply.started":"2024-06-10T17:18:16.389465Z","shell.execute_reply":"2024-06-10T17:19:19.022829Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"read batch 0\nread batch 1\nread batch 2\nread batch 3\nread batch 4\nread batch 5\n","output_type":"stream"}]},{"cell_type":"code","source":"# Feature extraction model using AlexNet\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        base_model = models.alexnet(weights='IMAGENET1K_V1')\n        self.feature_extractor = nn.Sequential(\n            *list(base_model.features), \n            # nn.Dropout(p=0.2),\n            nn.AdaptiveAvgPool2d((6, 6)),\n            nn.Flatten(),\n            *list(base_model.classifier[:-1])\n        )\n    \n    def forward(self, x):\n        x = self.feature_extractor(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:19:19.025166Z","iopub.execute_input":"2024-06-10T17:19:19.025497Z","iopub.status.idle":"2024-06-10T17:19:19.032076Z","shell.execute_reply.started":"2024-06-10T17:19:19.025463Z","shell.execute_reply":"2024-06-10T17:19:19.031252Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Number of tiles per bag\nFEATURE_DIM = 4096\n# MIL model\nclass MILModel(nn.Module):\n    def __init__(self):\n        super(MILModel, self).__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.classifier = nn.Sequential(\n            nn.Linear(FEATURE_DIM, 2048),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 1)\n        )\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        num_tiles = x.size(1)\n        x = x.view(-1, 3, 128, 128)\n        features = self.feature_extractor(x)\n        features = features.view(batch_size, num_tiles, -1)\n        aggregated_features, _ = torch.max(features, dim=1)\n        logits = self.classifier(aggregated_features)\n        output = torch.sigmoid(logits)\n        \n        return output\n    \n    def predict(self, x):\n        with torch.no_grad():\n            ypred_prob = self(x)\n            ypred = ypred_prob.clone().detach()\n            ypred[ypred < 0.5] = 0\n            ypred[ypred >= 0.5] = 1\n            return ypred_prob, ypred","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:19:19.033215Z","iopub.execute_input":"2024-06-10T17:19:19.033538Z","iopub.status.idle":"2024-06-10T17:19:19.049348Z","shell.execute_reply.started":"2024-06-10T17:19:19.033506Z","shell.execute_reply":"2024-06-10T17:19:19.048451Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Initialize the model\nmodel = MILModel().to(device)\nprint(model)\n\n# Define loss and optimizer\ncriterion = nn.BCELoss()\n# SGD in Case1 not well (predict prob is very close +-0.1?)\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:19:19.050367Z","iopub.execute_input":"2024-06-10T17:19:19.050618Z","iopub.status.idle":"2024-06-10T17:19:21.791195Z","shell.execute_reply.started":"2024-06-10T17:19:19.050596Z","shell.execute_reply":"2024-06-10T17:19:21.790259Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 156MB/s] \n","output_type":"stream"},{"name":"stdout","text":"MILModel(\n  (feature_extractor): FeatureExtractor(\n    (feature_extractor): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n      (1): ReLU(inplace=True)\n      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (4): ReLU(inplace=True)\n      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): ReLU(inplace=True)\n      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): ReLU(inplace=True)\n      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (13): AdaptiveAvgPool2d(output_size=(6, 6))\n      (14): Flatten(start_dim=1, end_dim=-1)\n      (15): Dropout(p=0.5, inplace=False)\n      (16): Linear(in_features=9216, out_features=4096, bias=True)\n      (17): ReLU(inplace=True)\n      (18): Dropout(p=0.5, inplace=False)\n      (19): Linear(in_features=4096, out_features=4096, bias=True)\n      (20): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=4096, out_features=2048, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=2048, out_features=1024, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=1024, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Case 3 each epoch go through every row\n\nnum_epochs = 20\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for tl in train_loader:\n        for i, (inputs, targets) in enumerate(tl):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    avg_train_loss = running_loss / (len(tl) * len(train_loader))\n    \n    model.eval()\n    val_loss = 0.0\n    for vl in val_loader:\n        with torch.no_grad():\n            for inputs, targets in vl:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n    avg_val_loss = val_loss / (len(vl) * len(val_loader))\n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\nprint('Training finished.')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:19:21.792370Z","iopub.execute_input":"2024-06-10T17:19:21.792656Z","iopub.status.idle":"2024-06-10T17:24:42.031851Z","shell.execute_reply.started":"2024-06-10T17:19:21.792632Z","shell.execute_reply":"2024-06-10T17:24:42.030913Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch [1/20], Train Loss: 0.7279, Val Loss: 0.6929\nEpoch [2/20], Train Loss: 0.6960, Val Loss: 0.6928\nEpoch [3/20], Train Loss: 0.6941, Val Loss: 0.6928\nEpoch [4/20], Train Loss: 0.6946, Val Loss: 0.6932\nEpoch [5/20], Train Loss: 0.6933, Val Loss: 0.6934\nEpoch [6/20], Train Loss: 0.6952, Val Loss: 0.6942\nEpoch [7/20], Train Loss: 0.6952, Val Loss: 0.6940\nEpoch [8/20], Train Loss: 0.6949, Val Loss: 0.6949\nEpoch [9/20], Train Loss: 0.6922, Val Loss: 0.6952\nEpoch [10/20], Train Loss: 0.6994, Val Loss: 0.6929\nEpoch [11/20], Train Loss: 0.6979, Val Loss: 0.6930\nEpoch [12/20], Train Loss: 0.6957, Val Loss: 0.6944\nEpoch [13/20], Train Loss: 0.6928, Val Loss: 0.6931\nEpoch [14/20], Train Loss: 0.6914, Val Loss: 0.6934\nEpoch [15/20], Train Loss: 0.6839, Val Loss: 0.6935\nEpoch [16/20], Train Loss: 0.6915, Val Loss: 0.6930\nEpoch [17/20], Train Loss: 0.6727, Val Loss: 0.6934\nEpoch [18/20], Train Loss: 0.6786, Val Loss: 0.6935\nEpoch [19/20], Train Loss: 0.6823, Val Loss: 0.6931\nEpoch [20/20], Train Loss: 0.6833, Val Loss: 0.6929\nTraining finished.\n","output_type":"stream"}]},{"cell_type":"code","source":"# train test and val test\nprob, lab = model.predict(trainX.to(device))\nprint(\"train acc:\",1 - torch.sum(torch.abs(lab.to(device) - trainY.to(device)))/lab.shape[0])\nprob, lab = model.predict(valX.to(device))\nprint(\"val acc:\", 1 -  torch.sum(torch.abs(lab.to(device) - valY.to(device)))/lab.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:24:42.034568Z","iopub.execute_input":"2024-06-10T17:24:42.034857Z","iopub.status.idle":"2024-06-10T17:24:44.000419Z","shell.execute_reply.started":"2024-06-10T17:24:42.034833Z","shell.execute_reply":"2024-06-10T17:24:43.999507Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"train acc: tensor(0.5250, device='cuda:0')\nval acc: tensor(0.6000, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# load test data and predict\n\npredictArray = np.array([])\n\nFileExtensionRemove = []\nfor i in range(int(len(testFileName)/DataSizeBatch)):\n    print(\"Predict Batch \", i)\n    \n    startIndex = i * DataSizeBatch\n    testX, Name = readData('test/', testFileName[startIndex:startIndex + DataSizeBatch])\n    FileExtensionRemove += Name\n    testX, _ = DataPreprocess(testX)\n    \n    pred_prob, pred_label = model.predict(testX.to(device))\n    predictArray = np.concatenate((predictArray, pred_label.cpu().detach().numpy().T[0]))\n\npredictArray = predictArray.astype(np.int64)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:24:44.001392Z","iopub.execute_input":"2024-06-10T17:24:44.001647Z","iopub.status.idle":"2024-06-10T17:25:26.564635Z","shell.execute_reply.started":"2024-06-10T17:24:44.001625Z","shell.execute_reply":"2024-06-10T17:25:26.563784Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Predict Batch  0\nPredict Batch  1\nPredict Batch  2\nPredict Batch  3\nPredict Batch  4\nPredict Batch  5\nPredict Batch  6\nPredict Batch  7\n","output_type":"stream"}]},{"cell_type":"code","source":"# to pandas\nsubmission = pd.DataFrame()\nsubmission[\"image_id\"] = FileExtensionRemove\nsubmission[\"y_pred\"] = predictArray","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:25:26.565759Z","iopub.execute_input":"2024-06-10T17:25:26.566041Z","iopub.status.idle":"2024-06-10T17:25:26.582196Z","shell.execute_reply.started":"2024-06-10T17:25:26.566018Z","shell.execute_reply":"2024-06-10T17:25:26.581245Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:25:26.583223Z","iopub.execute_input":"2024-06-10T17:25:26.583486Z","iopub.status.idle":"2024-06-10T17:25:26.607486Z","shell.execute_reply.started":"2024-06-10T17:25:26.583465Z","shell.execute_reply":"2024-06-10T17:25:26.606741Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                 image_id  y_pred\n0    7c08c8c0-0064-40c3-a7cd-dda2b4043347       1\n1    cb206033-5968-44f9-bfc8-dab33bb57bae       1\n2    355823c5-bc8e-4352-8892-10bf14ce866a       1\n3    7f6d455e-47a0-4529-954d-0cc432f63d50       1\n4    7a2304f3-f864-4c16-b683-468d87808661       1\n..                                    ...     ...\n195  4e4aef19-6408-46d9-9573-f8948c640e9d       1\n196  9ea39c64-6d6c-4ff6-ae08-71ebca779275       1\n197  e072b562-f2ef-42f7-8c60-ecf744a9469e       1\n198  80b8987c-70c4-4274-9b46-893e3877a8cd       1\n199  0a3d2366-2c62-489a-a9ec-069dbdfc07f3       1\n\n[200 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7c08c8c0-0064-40c3-a7cd-dda2b4043347</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cb206033-5968-44f9-bfc8-dab33bb57bae</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>355823c5-bc8e-4352-8892-10bf14ce866a</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7f6d455e-47a0-4529-954d-0cc432f63d50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7a2304f3-f864-4c16-b683-468d87808661</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>4e4aef19-6408-46d9-9573-f8948c640e9d</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>9ea39c64-6d6c-4ff6-ae08-71ebca779275</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>e072b562-f2ef-42f7-8c60-ecf744a9469e</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>80b8987c-70c4-4274-9b46-893e3877a8cd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0a3d2366-2c62-489a-a9ec-069dbdfc07f3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index = None)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:25:26.608385Z","iopub.execute_input":"2024-06-10T17:25:26.608612Z","iopub.status.idle":"2024-06-10T17:25:26.616938Z","shell.execute_reply.started":"2024-06-10T17:25:26.608592Z","shell.execute_reply":"2024-06-10T17:25:26.616092Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(pred_prob, pred_label)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:25:26.618102Z","iopub.execute_input":"2024-06-10T17:25:26.618663Z","iopub.status.idle":"2024-06-10T17:25:26.628735Z","shell.execute_reply.started":"2024-06-10T17:25:26.618640Z","shell.execute_reply":"2024-06-10T17:25:26.627945Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5038],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5038],\n        [0.5031],\n        [0.5032],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5031],\n        [0.5032],\n        [0.5034],\n        [0.5031],\n        [0.5031],\n        [0.5037],\n        [0.5031],\n        [0.5031]], device='cuda:0') tensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"np.unique(submission[\"image_id\"]).shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:25:26.629623Z","iopub.execute_input":"2024-06-10T17:25:26.629893Z","iopub.status.idle":"2024-06-10T17:25:26.636329Z","shell.execute_reply.started":"2024-06-10T17:25:26.629870Z","shell.execute_reply":"2024-06-10T17:25:26.635507Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(200,)"},"metadata":{}}]}]}